{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oti2M03ME05Z",
    "colab_type": "text"
   },
   "source": [
    "# Connect to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1TTIHcqUE9rV",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122.0
    },
    "outputId": "07c7ab4c-3449-4a45-8619-3d9fc18cb885",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573788119319E12,
     "user_tz": -480.0,
     "elapsed": 63967.0,
     "user": {
      "displayName": "Yantong Laii",
      "photoUrl": "",
      "userId": "11292323517047847832"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZhH-4t1GAwB",
    "colab_type": "text"
   },
   "source": [
    "### Enter the corresponding path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bn6lFF22FVxK",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "7ee7a4d4-7d36-41ea-94aa-1587868df8a9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573788299894E12,
     "user_tz": -480.0,
     "elapsed": 1759.0,
     "user": {
      "displayName": "Yantong Laii",
      "photoUrl": "",
      "userId": "11292323517047847832"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Pytorch Learning/Python3\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/'Pytorch Learning'/Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Gf-xcaa3FjIT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    },
    "outputId": "453b194c-e025-495e-9abf-742f16e898a1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573788307632E12,
     "user_tz": -480.0,
     "elapsed": 4908.0,
     "user": {
      "displayName": "Yantong Laii",
      "photoUrl": "",
      "userId": "11292323517047847832"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10.py\t     LeNet5.ckpt      __pycache__\n",
      "LeNet5_CIFAR10.ckpt  LeNet5_MNIST.py  RNN_sentiemnt_custom_dataset.ipynb\n",
      "LeNet5_CIFAR10.py    LeNet5.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-PIgNLJGjP8",
    "colab_type": "text"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w__1bpwUKhTX",
    "colab_type": "text"
   },
   "source": [
    "## 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iVbOY1MBGitU",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "21c06f46-131c-40ed-d8f7-577516c3e317",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573790852802E12,
     "user_tz": -480.0,
     "elapsed": 950654.0,
     "user": {
      "displayName": "Yantong Laii",
      "photoUrl": "",
      "userId": "11292323517047847832"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_data = 25000\n",
      "Number of test_data = 25000\n",
      "vars(train_data[0]) = {'labels': 'pos', 'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']}\n",
      "\n",
      "Number of train_data = 17500\n",
      "Number of valid_data = 7500\n",
      "Number of test_data = 25000\n",
      "\n",
      "vars(train_data[0]) =  {'labels': 'neg', 'text': ['This', 'movie', 'has', 'got', 'to', 'be', 'one', 'of', 'the', 'worst', 'I', 'have', 'ever', 'seen', 'make', 'it', 'to', 'DVD', '!', '!', '!', 'The', 'story', 'line', 'might', 'have', 'clicked', 'if', 'the', 'film', 'had', 'more', 'funding', 'and', 'writers', 'that', 'would', 'have', 'cut', 'the', 'nonsense', 'and', 'sickly', 'scenes', 'that', 'I', 'highly', 'caution', 'parents', 'on', '....', 'But', 'the', 'story', 'line', 'is', 'like', 'a', 'loose', 'cannon', '.', 'If', 'there', 'was', 'such', 'a', 'thing', 'as', 'a', 'drive', 'thru', 'movie', 'maker', '-', 'this', 'one', 'would', 'have', 'sprung', 'from', 'that', '.', 'It', 'reminded', 'me', 'a', 'lot', 'of', 'the', 'quickie', 'films', 'that', 'were', 'put', 'out', 'in', 'the', '1960', \"'s\", ',', 'poor', 'script', 'writing', 'and', 'filming', '.', '<', 'br', '/><br', '/>The', 'only', 'sensible', 'characters', 'in', 'the', 'whole', 'movie', 'was', 'the', 'bartender', 'and', 'beaver', '.', 'The', 'rest', 'of', 'the', 'film', ',', 'could', 'have', 'easily', 'been', 'made', 'by', 'middle', 'school', 'children', '.', 'I', 'give', 'this', 'film', 'a', 'rating', 'of', '1', 'as', 'it', 'is', 'truly', 'awful', 'and', 'left', 'my', 'entire', 'family', 'with', 'a', 'sense', 'of', 'being', 'cheated', '.', 'My', 'advice', '-', \"Don't\", 'Watch', 'It', '!', '!', '!']}\n",
      "INPUT_DIM = 101960\n",
      "OUTPUT_DIM = 2\n",
      "TEXT.pad_token = <pad>\n",
      "The model has 12507170 trainable parameters\n",
      "Epoch [1/10], Step [10/274], Loss: 0.6987\n",
      "Epoch [1/10], Step [20/274], Loss: 0.6983\n",
      "Epoch [1/10], Step [30/274], Loss: 0.7168\n",
      "Epoch [1/10], Step [40/274], Loss: 0.6758\n",
      "Epoch [1/10], Step [50/274], Loss: 0.6891\n",
      "Epoch [1/10], Step [60/274], Loss: 0.6349\n",
      "Epoch [1/10], Step [70/274], Loss: 0.6322\n",
      "Epoch [1/10], Step [80/274], Loss: 0.6438\n",
      "Epoch [1/10], Step [90/274], Loss: 0.6234\n",
      "Epoch [1/10], Step [100/274], Loss: 0.6406\n",
      "Epoch [1/10], Step [110/274], Loss: 0.5947\n",
      "Epoch [1/10], Step [120/274], Loss: 0.6639\n",
      "Epoch [1/10], Step [130/274], Loss: 0.6889\n",
      "Epoch [1/10], Step [140/274], Loss: 0.6571\n",
      "Epoch [1/10], Step [150/274], Loss: 0.5979\n",
      "Epoch [1/10], Step [160/274], Loss: 0.6141\n",
      "Epoch [1/10], Step [170/274], Loss: 0.6236\n",
      "Epoch [1/10], Step [180/274], Loss: 0.6632\n",
      "Epoch [1/10], Step [190/274], Loss: 0.5400\n",
      "Epoch [1/10], Step [200/274], Loss: 0.5018\n",
      "Epoch [1/10], Step [210/274], Loss: 0.5473\n",
      "Epoch [1/10], Step [220/274], Loss: 0.5094\n",
      "Epoch [1/10], Step [230/274], Loss: 0.5418\n",
      "Epoch [1/10], Step [240/274], Loss: 0.7299\n",
      "Epoch [1/10], Step [250/274], Loss: 0.5944\n",
      "Epoch [1/10], Step [260/274], Loss: 0.5555\n",
      "Epoch [1/10], Step [270/274], Loss: 0.6852\n",
      "total_loss = [0.6971074342727661, 0.6926023960113525, 0.6944335699081421, 0.7066554427146912, 0.7019243836402893, 0.6889471411705017, 0.6929795145988464, 0.7019293308258057, 0.6855855584144592, 0.698661744594574, 0.6957408785820007, 0.6753372550010681, 0.6781378984451294, 0.6833226680755615, 0.6860979795455933, 0.6688020825386047, 0.6864996552467346, 0.6832773685455322, 0.6829872727394104, 0.6983078718185425, 0.6946616172790527, 0.6709185838699341, 0.6521228551864624, 0.6768783926963806, 0.6656041145324707, 0.6960062384605408, 0.716317892074585, 0.7166517376899719, 0.6420713663101196, 0.7167977094650269, 0.6892359256744385, 0.6465011835098267, 0.6465982794761658, 0.6339173316955566, 0.6445428133010864, 0.6695842146873474, 0.6322795152664185, 0.7049623131752014, 0.5860766768455505, 0.6758105158805847, 0.6486920118331909, 0.5894630551338196, 0.6686739921569824, 0.6388062834739685, 0.6202784180641174, 0.658543050289154, 0.6782796382904053, 0.6688055396080017, 0.7516551613807678, 0.6891016364097595, 0.7048838138580322, 0.6795670390129089, 0.6505685448646545, 0.6535006165504456, 0.6466861963272095, 0.6551164388656616, 0.6544696688652039, 0.6338119506835938, 0.6461348533630371, 0.6349027752876282, 0.6601056456565857, 0.6893752813339233, 0.6024100184440613, 0.6652839183807373, 0.5728732943534851, 0.657556414604187, 0.5753477811813354, 0.7358380556106567, 0.6338179111480713, 0.6321905255317688, 0.676128625869751, 0.6149939894676208, 0.5514288544654846, 0.6030442118644714, 0.5982106328010559, 0.6872338056564331, 0.6197301745414734, 0.7009231448173523, 0.5900479555130005, 0.6437564492225647, 0.6061327457427979, 0.8050617575645447, 0.6487751007080078, 0.6397981643676758, 0.6165308356285095, 0.5981706976890564, 0.6400647759437561, 0.6432872414588928, 0.610061526298523, 0.6233929991722107, 0.6524252891540527, 0.5908487439155579, 0.7237083315849304, 0.6610915660858154, 0.6340770125389099, 0.6088675260543823, 0.6588161587715149, 0.6096521019935608, 0.703386664390564, 0.640571117401123, 0.6615618467330933, 0.6260469555854797, 0.6959362626075745, 0.6202113032341003, 0.6629969477653503, 0.664683997631073, 0.5871626734733582, 0.5671889185905457, 0.7066330313682556, 0.5947399139404297, 0.627225399017334, 0.5567182302474976, 0.6152864694595337, 0.6634696125984192, 0.6766296625137329, 0.6181678175926208, 0.6767902970314026, 0.5982977747917175, 0.6445754766464233, 0.6639377474784851, 0.7310929894447327, 0.6279425024986267, 0.5875493884086609, 0.681847095489502, 0.6235159635543823, 0.6358555555343628, 0.6136050224304199, 0.6214267611503601, 0.581305205821991, 0.688883364200592, 0.5617184042930603, 0.57974773645401, 0.601402997970581, 0.637465238571167, 0.6805586814880371, 0.5685423016548157, 0.7453263401985168, 0.6365121006965637, 0.6820932030677795, 0.657105028629303, 0.6605360507965088, 0.603974461555481, 0.5757666826248169, 0.6355116367340088, 0.51806640625, 0.6267766356468201, 0.5882217884063721, 0.6107599139213562, 0.6551227569580078, 0.5978877544403076, 0.5920547246932983, 0.5829037427902222, 0.5001912117004395, 0.5022140741348267, 0.6009329557418823, 0.6681109070777893, 0.6321532130241394, 0.6603739261627197, 0.6315038800239563, 0.6140560507774353, 0.6603764295578003, 0.5453557968139648, 0.5928565263748169, 0.5650638937950134, 0.6030523777008057, 0.5922436118125916, 0.632003128528595, 0.6490898132324219, 0.6131021976470947, 0.6236297488212585, 0.6659315228462219, 0.5992341637611389, 0.654501736164093, 0.5516380071640015, 0.6442763209342957, 0.564167857170105, 0.5792584419250488, 0.5516301393508911, 0.5717931389808655, 0.6632258296012878, 0.6167759299278259, 0.6906720995903015, 0.5939365029335022, 0.6380185484886169, 0.5874305963516235, 0.7838110327720642, 0.5161917209625244, 0.5629778504371643, 0.647017776966095, 0.5400180220603943, 0.5512973070144653, 0.6687074303627014, 0.5162502527236938, 0.6339434385299683, 0.5776146650314331, 0.5389363765716553, 0.5725247263908386, 0.5866126418113708, 0.5597694516181946, 0.5018121600151062, 0.5247613787651062, 0.5003108978271484, 0.556298017501831, 0.6523593068122864, 0.5989117622375488, 0.6234193444252014, 0.5823359489440918, 0.5033309459686279, 0.6026250123977661, 0.5472639799118042, 0.5918102264404297, 0.5981362462043762, 0.5174161791801453, 0.5354424118995667, 0.6049558520317078, 0.5127808451652527, 0.571158230304718, 0.6264204382896423, 0.6362601518630981, 0.5094220638275146, 0.5285153388977051, 0.5405282378196716, 0.6824774146080017, 0.6447275280952454, 0.6590782999992371, 0.6238442063331604, 0.5796735286712646, 0.5212452411651611, 0.6112959384918213, 0.5418224334716797, 0.6165091395378113, 0.5035126805305481, 0.42698004841804504, 0.5508357882499695, 0.5072256922721863, 0.551746129989624, 0.5091281533241272, 0.588951051235199, 0.5648632645606995, 0.7299354672431946, 0.5746311545372009, 0.6292704343795776, 0.6250686049461365, 0.5570166707038879, 0.57636559009552, 0.6633345484733582, 0.5947946906089783, 0.6124732494354248, 0.5530725121498108, 0.5943741202354431, 0.598467230796814, 0.5788692831993103, 0.6146332621574402, 0.6079219579696655, 0.6026608347892761, 0.5726261138916016, 0.5063636898994446, 0.567431628704071, 0.52598637342453, 0.5554889440536499, 0.5879263877868652, 0.6842647790908813, 0.5511415600776672, 0.5601067543029785, 0.5586937665939331, 0.5751025676727295, 0.641950786113739, 0.5340954661369324, 0.5856214165687561, 0.6851716637611389, 0.7007432579994202, 0.5927115678787231, 0.48448726534843445, 0.49917933344841003]\n",
      "\n",
      "Epoch [2/10], Step [10/274], Loss: 0.4587\n",
      "Epoch [2/10], Step [20/274], Loss: 0.5747\n",
      "Epoch [2/10], Step [30/274], Loss: 0.4832\n",
      "Epoch [2/10], Step [40/274], Loss: 0.5257\n",
      "Epoch [2/10], Step [50/274], Loss: 0.4758\n",
      "Epoch [2/10], Step [60/274], Loss: 0.5297\n",
      "Epoch [2/10], Step [70/274], Loss: 0.4918\n",
      "Epoch [2/10], Step [80/274], Loss: 0.3814\n",
      "Epoch [2/10], Step [90/274], Loss: 0.4688\n",
      "Epoch [2/10], Step [100/274], Loss: 0.4301\n",
      "Epoch [2/10], Step [110/274], Loss: 0.4122\n",
      "Epoch [2/10], Step [120/274], Loss: 0.5138\n",
      "Epoch [2/10], Step [130/274], Loss: 0.5403\n",
      "Epoch [2/10], Step [140/274], Loss: 0.4974\n",
      "Epoch [2/10], Step [150/274], Loss: 0.3742\n",
      "Epoch [2/10], Step [160/274], Loss: 0.4830\n",
      "Epoch [2/10], Step [170/274], Loss: 0.5190\n",
      "Epoch [2/10], Step [180/274], Loss: 0.2996\n",
      "Epoch [2/10], Step [190/274], Loss: 0.5970\n",
      "Epoch [2/10], Step [200/274], Loss: 0.6125\n",
      "Epoch [2/10], Step [210/274], Loss: 0.5379\n",
      "Epoch [2/10], Step [220/274], Loss: 0.5281\n",
      "Epoch [2/10], Step [230/274], Loss: 0.5557\n",
      "Epoch [2/10], Step [240/274], Loss: 0.6218\n",
      "Epoch [2/10], Step [250/274], Loss: 0.3978\n",
      "Epoch [2/10], Step [260/274], Loss: 0.4659\n",
      "Epoch [2/10], Step [270/274], Loss: 0.4171\n",
      "total_loss = [0.5427555441856384, 0.5245118737220764, 0.48082077503204346, 0.43198537826538086, 0.46893468499183655, 0.4375477135181427, 0.48899033665657043, 0.4112669825553894, 0.46969079971313477, 0.4586581587791443, 0.4582117199897766, 0.45581427216529846, 0.5545896291732788, 0.4643407166004181, 0.49892786145210266, 0.545430600643158, 0.500906765460968, 0.5061283111572266, 0.5536040663719177, 0.5746896266937256, 0.46826303005218506, 0.49405738711357117, 0.4960286319255829, 0.5381283164024353, 0.6256451606750488, 0.5029925107955933, 0.5650417804718018, 0.579744279384613, 0.46890726685523987, 0.48323071002960205, 0.5255210399627686, 0.5491344928741455, 0.5491231679916382, 0.5158360004425049, 0.5096355676651001, 0.587088942527771, 0.5437648892402649, 0.5102701187133789, 0.5111579895019531, 0.5257429480552673, 0.4438360333442688, 0.48795437812805176, 0.4496515095233917, 0.6042647361755371, 0.4510643780231476, 0.5763530731201172, 0.5049358010292053, 0.5000920295715332, 0.6018160581588745, 0.47579142451286316, 0.5082890391349792, 0.56830233335495, 0.5306034684181213, 0.5090477466583252, 0.4718135595321655, 0.4530382752418518, 0.5594934225082397, 0.44013726711273193, 0.4518234133720398, 0.5296922922134399, 0.8036509156227112, 0.5024813413619995, 0.5419127941131592, 0.48549169301986694, 0.519069254398346, 0.48385223746299744, 0.5657740831375122, 0.4539058804512024, 0.4401097595691681, 0.4918059706687927, 0.5015600919723511, 0.4319286048412323, 0.3981228768825531, 0.42365941405296326, 0.5866063237190247, 0.6020906567573547, 0.42673540115356445, 0.6653425693511963, 0.42047348618507385, 0.38138628005981445, 0.48023927211761475, 0.39287158846855164, 0.5023459196090698, 0.5409649014472961, 0.5245899558067322, 0.4740840792655945, 0.4426238536834717, 0.5430710315704346, 0.5857684016227722, 0.46876639127731323, 0.5058144330978394, 0.4194738268852234, 0.5767338275909424, 0.42413294315338135, 0.4292231798171997, 0.5101028680801392, 0.6438192129135132, 0.33354872465133667, 0.6246074438095093, 0.4301397502422333, 0.45435023307800293, 0.5617566108703613, 0.6132928133010864, 0.4332661032676697, 0.4773104190826416, 0.5415228605270386, 0.4909858703613281, 0.41421857476234436, 0.5725499987602234, 0.4121602475643158, 0.4652271270751953, 0.3959476947784424, 0.5112869143486023, 0.5201262831687927, 0.4958396852016449, 0.37387925386428833, 0.5126132369041443, 0.46121180057525635, 0.45755118131637573, 0.513757586479187, 0.40173542499542236, 0.5621182322502136, 0.5329719185829163, 0.40963616967201233, 0.46358129382133484, 0.564940333366394, 0.4964338541030884, 0.5271503925323486, 0.4112183749675751, 0.540308952331543, 0.4987804889678955, 0.4543524980545044, 0.577941358089447, 0.45557188987731934, 0.5685070157051086, 0.4479472041130066, 0.48305100202560425, 0.4369650185108185, 0.46176114678382874, 0.49735504388809204, 0.592570424079895, 0.40761813521385193, 0.5204980969429016, 0.6188362240791321, 0.3637355864048004, 0.5251817107200623, 0.3688165545463562, 0.45497313141822815, 0.5173988938331604, 0.3741688132286072, 0.3994963765144348, 0.4931710958480835, 0.37330663204193115, 0.49742987751960754, 0.35246479511260986, 0.5066320896148682, 0.538249671459198, 0.42242082953453064, 0.48936927318573, 0.4830487370491028, 0.5161482691764832, 0.41251569986343384, 0.523788571357727, 0.38666221499443054, 0.5184252262115479, 0.5100798010826111, 0.48418688774108887, 0.46640950441360474, 0.4032007157802582, 0.5190081000328064, 0.5462775826454163, 0.41126707196235657, 0.34905529022216797, 0.36332306265830994, 0.371049165725708, 0.44873592257499695, 0.40688279271125793, 0.47847259044647217, 0.4125659763813019, 0.29956385493278503, 0.4089203178882599, 0.33560675382614136, 0.411763072013855, 0.37383371591567993, 0.30172792077064514, 0.4451938271522522, 0.5874167680740356, 0.6191343069076538, 0.5536068081855774, 0.5969517827033997, 0.8110517263412476, 0.6836233735084534, 0.5831912755966187, 0.7240316867828369, 0.6457650661468506, 0.7058439254760742, 0.5607871413230896, 0.5745849609375, 0.5309678316116333, 0.6125311851501465, 0.538505494594574, 0.6311026811599731, 0.6943226456642151, 0.4596024453639984, 0.5678379535675049, 0.4879196882247925, 0.6245592832565308, 0.7016210556030273, 0.5742309093475342, 0.5379270315170288, 0.6073088049888611, 0.519318699836731, 0.5910178422927856, 0.5234200358390808, 0.43910500407218933, 0.506227970123291, 0.4776627719402313, 0.512296199798584, 0.5909674167633057, 0.528104841709137, 0.4237877428531647, 0.402375191450119, 0.45900022983551025, 0.5225788354873657, 0.5821899771690369, 0.5720042586326599, 0.46039626002311707, 0.6379820108413696, 0.42524054646492004, 0.5557236671447754, 0.486296683549881, 0.6255106329917908, 0.45135635137557983, 0.5100095868110657, 0.4864825904369354, 0.5036492943763733, 0.378254234790802, 0.6970854997634888, 0.5453509092330933, 0.6218340396881104, 0.5225881934165955, 0.41885632276535034, 0.5126921534538269, 0.5745973587036133, 0.4816165268421173, 0.5590124726295471, 0.5447476506233215, 0.4988275468349457, 0.5156639814376831, 0.39777708053588867, 0.4814917743206024, 0.6274764537811279, 0.5163370966911316, 0.6120597720146179, 0.5765875577926636, 0.49974316358566284, 0.6277306079864502, 0.5113418102264404, 0.42991456389427185, 0.4659171402454376, 0.5432453155517578, 0.5419741272926331, 0.6047347187995911, 0.38635143637657166, 0.46635034680366516, 0.7866368889808655, 0.5676043629646301, 0.3583299219608307, 0.43356356024742126, 0.4171140491962433, 0.47921448945999146, 0.5670109987258911, 0.5825386047363281, 0.509921669960022]\n",
      "\n",
      "Epoch [3/10], Step [10/274], Loss: 0.4019\n",
      "Epoch [3/10], Step [20/274], Loss: 0.3113\n",
      "Epoch [3/10], Step [30/274], Loss: 0.4484\n",
      "Epoch [3/10], Step [40/274], Loss: 0.3768\n",
      "Epoch [3/10], Step [50/274], Loss: 0.5838\n",
      "Epoch [3/10], Step [60/274], Loss: 0.3818\n",
      "Epoch [3/10], Step [70/274], Loss: 0.3013\n",
      "Epoch [3/10], Step [80/274], Loss: 0.3571\n",
      "Epoch [3/10], Step [90/274], Loss: 0.4244\n",
      "Epoch [3/10], Step [100/274], Loss: 0.4305\n",
      "Epoch [3/10], Step [110/274], Loss: 0.4389\n",
      "Epoch [3/10], Step [120/274], Loss: 0.4468\n",
      "Epoch [3/10], Step [130/274], Loss: 0.3909\n",
      "Epoch [3/10], Step [140/274], Loss: 0.2975\n",
      "Epoch [3/10], Step [150/274], Loss: 0.4397\n",
      "Epoch [3/10], Step [160/274], Loss: 0.4504\n",
      "Epoch [3/10], Step [170/274], Loss: 0.3631\n",
      "Epoch [3/10], Step [180/274], Loss: 0.5210\n",
      "Epoch [3/10], Step [190/274], Loss: 0.3261\n",
      "Epoch [3/10], Step [200/274], Loss: 0.4318\n",
      "Epoch [3/10], Step [210/274], Loss: 0.3160\n",
      "Epoch [3/10], Step [220/274], Loss: 0.5310\n",
      "Epoch [3/10], Step [230/274], Loss: 0.3514\n",
      "Epoch [3/10], Step [240/274], Loss: 0.3352\n",
      "Epoch [3/10], Step [250/274], Loss: 0.4046\n",
      "Epoch [3/10], Step [260/274], Loss: 0.2693\n",
      "Epoch [3/10], Step [270/274], Loss: 0.2716\n",
      "total_loss = [0.41863977909088135, 0.3904782235622406, 0.4081379175186157, 0.4969605803489685, 0.5566071271896362, 0.5490790605545044, 0.5280832052230835, 0.5055269002914429, 0.34163233637809753, 0.4018853008747101, 0.44067490100860596, 0.4380737543106079, 0.4083482027053833, 0.46731165051460266, 0.42482563853263855, 0.47012051939964294, 0.4197739064693451, 0.447034627199173, 0.4444224536418915, 0.3112926781177521, 0.39774248003959656, 0.33843785524368286, 0.3397730886936188, 0.3509833514690399, 0.3287605047225952, 0.3363753855228424, 0.2902762293815613, 0.3618290424346924, 0.3242664039134979, 0.4483999013900757, 0.25006967782974243, 0.31898146867752075, 0.3014862537384033, 0.25896525382995605, 0.44200173020362854, 0.32367202639579773, 0.3363865911960602, 0.42395782470703125, 0.4176793396472931, 0.3768009841442108, 0.35034140944480896, 0.36240044236183167, 0.36972951889038086, 0.6437504887580872, 0.534140944480896, 0.46787339448928833, 0.38250744342803955, 0.4032319188117981, 0.47698450088500977, 0.5837826132774353, 0.4935398995876312, 0.35565632581710815, 0.4080139100551605, 0.3447466194629669, 0.4389524757862091, 0.3967507481575012, 0.3904772400856018, 0.3860427737236023, 0.3228217661380768, 0.38178086280822754, 0.40867316722869873, 0.41824445128440857, 0.28433355689048767, 0.42831188440322876, 0.34469538927078247, 0.2981666326522827, 0.37474551796913147, 0.5780118703842163, 0.478788822889328, 0.30125242471694946, 0.5152950882911682, 0.41026571393013, 0.39277610182762146, 0.3845461905002594, 0.4077812731266022, 0.37046894431114197, 0.45766741037368774, 0.4680662453174591, 0.3838091194629669, 0.35706332325935364, 0.3050714135169983, 0.43755167722702026, 0.42816615104675293, 0.44164472818374634, 0.40800580382347107, 0.407124787569046, 0.33391547203063965, 0.4001637101173401, 0.4997192621231079, 0.42442581057548523, 0.4377731382846832, 0.4181176424026489, 0.33033016324043274, 0.3301810026168823, 0.47566908597946167, 0.4449804723262787, 0.4142349064350128, 0.4362414479255676, 0.4333091676235199, 0.43046507239341736, 0.36705076694488525, 0.44892799854278564, 0.37257999181747437, 0.2762829065322876, 0.3020000755786896, 0.48143741488456726, 0.40004679560661316, 0.3635605275630951, 0.30233365297317505, 0.43894052505493164, 0.37640002369880676, 0.32699471712112427, 0.4979192316532135, 0.42234838008880615, 0.2826693058013916, 0.38786107301712036, 0.3354194760322571, 0.28210482001304626, 0.4684070944786072, 0.4468221068382263, 0.3540460765361786, 0.47233110666275024, 0.39662492275238037, 0.4477529525756836, 0.40955308079719543, 0.4581244885921478, 0.44162896275520325, 0.3951755464076996, 0.3056230843067169, 0.3909348249435425, 0.3956359326839447, 0.2897704839706421, 0.38032853603363037, 0.51023930311203, 0.2617151737213135, 0.4694737493991852, 0.5585523843765259, 0.22272714972496033, 0.4332071840763092, 0.29752489924430847, 0.49088484048843384, 0.45118677616119385, 0.47983571887016296, 0.3828312158584595, 0.30834540724754333, 0.3457423150539398, 0.4701897203922272, 0.49068933725357056, 0.7111253142356873, 0.43967655301094055, 0.39137232303619385, 0.4848147928714752, 0.33665066957473755, 0.3809066712856293, 0.46190643310546875, 0.4737071692943573, 0.4727994203567505, 0.39879733324050903, 0.3856579065322876, 0.4503965973854065, 0.36555016040802, 0.3173498809337616, 0.40809813141822815, 0.3324131369590759, 0.369555801153183, 0.5427366495132446, 0.36909738183021545, 0.5393181443214417, 0.4049590229988098, 0.3630632758140564, 0.2975502610206604, 0.42541056871414185, 0.4978213608264923, 0.4409998953342438, 0.3321703374385834, 0.44366520643234253, 0.42980045080184937, 0.3439784646034241, 0.3352585434913635, 0.5209629535675049, 0.3956092596054077, 0.3620465099811554, 0.43935251235961914, 0.33342084288597107, 0.5314613580703735, 0.322532057762146, 0.4348333179950714, 0.29552754759788513, 0.4657665491104126, 0.326121062040329, 0.2761612832546234, 0.35703808069229126, 0.37729430198669434, 0.3624183237552643, 0.43544286489486694, 0.3343227505683899, 0.3676915466785431, 0.2945239841938019, 0.3686496317386627, 0.4318060278892517, 0.36007291078567505, 0.4530094563961029, 0.4142966866493225, 0.39375388622283936, 0.39299097657203674, 0.3605964779853821, 0.3496178388595581, 0.4695127606391907, 0.3939976096153259, 0.31599143147468567, 0.2547077238559723, 0.45165500044822693, 0.4258681535720825, 0.3841322958469391, 0.41898834705352783, 0.4188646078109741, 0.3861426115036011, 0.349152535200119, 0.4550337791442871, 0.5309731960296631, 0.4897037744522095, 0.41765910387039185, 0.4193919897079468, 0.26346442103385925, 0.3553837835788727, 0.41831743717193604, 0.504846453666687, 0.3786502778530121, 0.34437939524650574, 0.3514261245727539, 0.3411494195461273, 0.3088829219341278, 0.538912832736969, 0.3596707284450531, 0.34621986746788025, 0.2763422429561615, 0.3742734491825104, 0.5595237016677856, 0.3894376754760742, 0.33518996834754944, 0.4246169328689575, 0.37933915853500366, 0.4452342092990875, 0.48161056637763977, 0.38235199451446533, 0.2736366391181946, 0.37802639603614807, 0.27719470858573914, 0.40540167689323425, 0.4045585095882416, 0.27290084958076477, 0.35702142119407654, 0.2142363339662552, 0.4727512300014496, 0.4857653081417084, 0.456586092710495, 0.15415047109127045, 0.4512427747249603, 0.33371707797050476, 0.26934435963630676, 0.43769076466560364, 0.3930334746837616, 0.2844766080379486, 0.440104216337204, 0.353940486907959, 0.33552947640419006, 0.310979962348938, 0.5460819005966187, 0.288289338350296, 0.2715902030467987, 0.32583072781562805, 0.2827180027961731, 0.368825227022171, 0.3992096781730652]\n",
      "\n",
      "Epoch [4/10], Step [10/274], Loss: 0.3013\n",
      "Epoch [4/10], Step [20/274], Loss: 0.3546\n",
      "Epoch [4/10], Step [30/274], Loss: 0.2532\n",
      "Epoch [4/10], Step [40/274], Loss: 0.2706\n",
      "Epoch [4/10], Step [50/274], Loss: 0.2605\n",
      "Epoch [4/10], Step [60/274], Loss: 0.2411\n",
      "Epoch [4/10], Step [70/274], Loss: 0.3615\n",
      "Epoch [4/10], Step [80/274], Loss: 0.2332\n",
      "Epoch [4/10], Step [90/274], Loss: 0.3693\n",
      "Epoch [4/10], Step [100/274], Loss: 0.1757\n",
      "Epoch [4/10], Step [110/274], Loss: 0.2432\n",
      "Epoch [4/10], Step [120/274], Loss: 0.3390\n",
      "Epoch [4/10], Step [130/274], Loss: 0.2607\n",
      "Epoch [4/10], Step [140/274], Loss: 0.7763\n",
      "Epoch [4/10], Step [150/274], Loss: 0.4813\n",
      "Epoch [4/10], Step [160/274], Loss: 0.3273\n",
      "Epoch [4/10], Step [170/274], Loss: 0.4986\n",
      "Epoch [4/10], Step [180/274], Loss: 0.3314\n",
      "Epoch [4/10], Step [190/274], Loss: 0.3885\n",
      "Epoch [4/10], Step [200/274], Loss: 0.3490\n",
      "Epoch [4/10], Step [210/274], Loss: 0.3250\n",
      "Epoch [4/10], Step [220/274], Loss: 0.3009\n",
      "Epoch [4/10], Step [230/274], Loss: 0.1609\n",
      "Epoch [4/10], Step [240/274], Loss: 0.2178\n",
      "Epoch [4/10], Step [250/274], Loss: 0.2879\n",
      "Epoch [4/10], Step [260/274], Loss: 0.3056\n",
      "Epoch [4/10], Step [270/274], Loss: 0.2309\n",
      "total_loss = [0.33514195680618286, 0.28480029106140137, 0.40816551446914673, 0.4135897755622864, 0.23975934088230133, 0.4180936813354492, 0.3178027868270874, 0.3743060827255249, 0.4013659656047821, 0.3012641966342926, 0.3153119385242462, 0.3059709966182709, 0.3277045488357544, 0.2871485948562622, 0.2903062701225281, 0.3332096338272095, 0.40567296743392944, 0.42534974217414856, 0.43960142135620117, 0.3546009659767151, 0.4724063277244568, 0.27593985199928284, 0.2996220886707306, 0.26108279824256897, 0.4081069231033325, 0.2999020218849182, 0.3542448878288269, 0.42886245250701904, 0.4374518096446991, 0.2532247006893158, 0.4348234534263611, 0.2440420538187027, 0.36840277910232544, 0.3204801082611084, 0.3287515342235565, 0.38101089000701904, 0.22869758307933807, 0.48595911264419556, 0.30348968505859375, 0.2705939710140228, 0.3907141089439392, 0.42847996950149536, 0.23573161661624908, 0.4222816526889801, 0.35867562890052795, 0.34874340891838074, 0.4649633765220642, 0.3941958546638489, 0.2895352840423584, 0.260489284992218, 0.26608577370643616, 0.2914433777332306, 0.32196691632270813, 0.33543285727500916, 0.40339165925979614, 0.35513392090797424, 0.25913482904434204, 0.2514967620372772, 0.3231482207775116, 0.2410620003938675, 0.39605629444122314, 0.14609883725643158, 0.21998949348926544, 0.17590221762657166, 0.24315257370471954, 0.23975801467895508, 0.4075268507003784, 0.22110380232334137, 0.20949217677116394, 0.36150190234184265, 0.2710552215576172, 0.12057644128799438, 0.26760220527648926, 0.24690228700637817, 0.28186696767807007, 0.24006085097789764, 0.32129961252212524, 0.3829880356788635, 0.3927581012248993, 0.23318170011043549, 0.30077511072158813, 0.2600499987602234, 0.15864495933055878, 0.3321719169616699, 0.2803506553173065, 0.18575060367584229, 0.20811223983764648, 0.30575332045555115, 0.35072970390319824, 0.36931589245796204, 0.20604851841926575, 0.19217459857463837, 0.38719049096107483, 0.1694163978099823, 0.19311398267745972, 0.31512022018432617, 0.22974151372909546, 0.31114569306373596, 0.2795936167240143, 0.17566615343093872, 0.25445377826690674, 0.3204529881477356, 0.4010300636291504, 0.22011759877204895, 0.29598334431648254, 0.23459383845329285, 0.2796870172023773, 0.294166624546051, 0.2908388674259186, 0.24323125183582306, 0.20453040301799774, 0.28181496262550354, 0.40637752413749695, 0.4219907522201538, 0.2938189208507538, 0.3964976966381073, 0.3276369571685791, 0.25770094990730286, 0.3198543190956116, 0.33900296688079834, 0.32063886523246765, 0.2235453724861145, 0.2506610155105591, 0.27677974104881287, 0.3226080536842346, 0.2912464737892151, 0.27786916494369507, 0.2786811590194702, 0.2569536566734314, 0.2607138454914093, 0.19953753054141998, 0.22126609086990356, 0.20254454016685486, 0.3245522379875183, 0.26134243607521057, 0.16659411787986755, 0.27977707982063293, 0.30515187978744507, 0.5486818552017212, 0.7763079404830933, 0.7799347639083862, 0.5257066488265991, 0.515408456325531, 0.4665384888648987, 0.4008725583553314, 0.337626188993454, 0.5765302777290344, 0.7113163471221924, 0.8303279280662537, 0.4812711179256439, 0.6155369877815247, 0.46236452460289, 0.39931607246398926, 0.39333370327949524, 0.4162505865097046, 0.4480002522468567, 0.4438796937465668, 0.3480096161365509, 0.37230077385902405, 0.3273084759712219, 0.29109975695610046, 0.24479059875011444, 0.31134405732154846, 0.2905196249485016, 0.38908758759498596, 0.3215261995792389, 0.46970999240875244, 0.21814538538455963, 0.505013108253479, 0.498595654964447, 0.2964262366294861, 0.2737959921360016, 0.3652626574039459, 0.4435589909553528, 0.28696924448013306, 0.24934855103492737, 0.37201935052871704, 0.3584625720977783, 0.26683375239372253, 0.3313865065574646, 0.27909159660339355, 0.2420957386493683, 0.3525661826133728, 0.42862093448638916, 0.19711154699325562, 0.2901286482810974, 0.26502275466918945, 0.192787304520607, 0.5366706252098083, 0.3885008692741394, 0.2580249011516571, 0.7582191228866577, 0.6089237332344055, 0.6011687517166138, 0.2974514067173004, 0.3789093494415283, 0.3312939405441284, 0.29922571778297424, 0.2551615536212921, 0.3490045368671417, 0.37669163942337036, 0.3884752094745636, 0.37998270988464355, 0.4118764400482178, 0.38835009932518005, 0.32303401827812195, 0.3726290166378021, 0.350678026676178, 0.2741873264312744, 0.3250004053115845, 0.2646285891532898, 0.33215636014938354, 0.2895556092262268, 0.3121647834777832, 0.25362876057624817, 0.3237338960170746, 0.33839237689971924, 0.2584611177444458, 0.39175575971603394, 0.30088138580322266, 0.2509666085243225, 0.27651485800743103, 0.37158483266830444, 0.26075756549835205, 0.1736196130514145, 0.2876843512058258, 0.3576910197734833, 0.33908721804618835, 0.2370562106370926, 0.16090700030326843, 0.3511991500854492, 0.2843047082424164, 0.31304827332496643, 0.26042640209198, 0.3174208998680115, 0.29770511388778687, 0.3311651647090912, 0.20106393098831177, 0.26332011818885803, 0.2177755981683731, 0.21547530591487885, 0.1691211760044098, 0.2201828509569168, 0.29535505175590515, 0.2493557184934616, 0.32187244296073914, 0.358230859041214, 0.4741085469722748, 0.6296854019165039, 0.2878737449645996, 0.3213707208633423, 0.17439700663089752, 0.4653398096561432, 0.463717520236969, 0.6779082417488098, 0.281887024641037, 0.2862878441810608, 0.2838895320892334, 0.4464157819747925, 0.3056248426437378, 0.3067018687725067, 0.29212379455566406, 0.4282768964767456, 0.33845946192741394, 0.2811262905597687, 0.42569881677627563, 0.28916460275650024, 0.19057773053646088, 0.3921583294868469, 0.23087027668952942, 0.37939825654029846, 0.33790501952171326, 0.2245817482471466, 0.1945512294769287]\n",
      "\n",
      "Epoch [5/10], Step [10/274], Loss: 0.2390\n",
      "Epoch [5/10], Step [20/274], Loss: 0.1582\n",
      "Epoch [5/10], Step [30/274], Loss: 0.3846\n",
      "Epoch [5/10], Step [40/274], Loss: 0.1920\n",
      "Epoch [5/10], Step [50/274], Loss: 0.2626\n",
      "Epoch [5/10], Step [60/274], Loss: 0.2354\n",
      "Epoch [5/10], Step [70/274], Loss: 0.2763\n",
      "Epoch [5/10], Step [80/274], Loss: 0.2363\n",
      "Epoch [5/10], Step [90/274], Loss: 0.1456\n",
      "Epoch [5/10], Step [100/274], Loss: 0.2448\n",
      "Epoch [5/10], Step [110/274], Loss: 0.3284\n",
      "Epoch [5/10], Step [120/274], Loss: 0.2864\n",
      "Epoch [5/10], Step [130/274], Loss: 0.1754\n",
      "Epoch [5/10], Step [140/274], Loss: 0.2915\n",
      "Epoch [5/10], Step [150/274], Loss: 0.2582\n",
      "Epoch [5/10], Step [160/274], Loss: 0.1835\n",
      "Epoch [5/10], Step [170/274], Loss: 0.1392\n",
      "Epoch [5/10], Step [180/274], Loss: 0.3069\n",
      "Epoch [5/10], Step [190/274], Loss: 0.1727\n",
      "Epoch [5/10], Step [200/274], Loss: 0.1847\n",
      "Epoch [5/10], Step [210/274], Loss: 0.2043\n",
      "Epoch [5/10], Step [220/274], Loss: 0.1825\n",
      "Epoch [5/10], Step [230/274], Loss: 0.2045\n",
      "Epoch [5/10], Step [240/274], Loss: 0.3658\n",
      "Epoch [5/10], Step [250/274], Loss: 0.2303\n",
      "Epoch [5/10], Step [260/274], Loss: 0.1602\n",
      "Epoch [5/10], Step [270/274], Loss: 0.2397\n",
      "total_loss = [0.28585484623908997, 0.16928763687610626, 0.27524763345718384, 0.36665382981300354, 0.2557452619075775, 0.19715023040771484, 0.2461950033903122, 0.17518922686576843, 0.3270857036113739, 0.23903566598892212, 0.3163396418094635, 0.19957585632801056, 0.25578656792640686, 0.15238122642040253, 0.14935266971588135, 0.1660497933626175, 0.14477397501468658, 0.296660453081131, 0.2891811430454254, 0.15820536017417908, 0.1352735161781311, 0.30176085233688354, 0.18931862711906433, 0.2888406217098236, 0.2023945450782776, 0.3272837996482849, 0.18067045509815216, 0.442732036113739, 0.29854506254196167, 0.38455986976623535, 0.21693547070026398, 0.28731122612953186, 0.21337848901748657, 0.25375816226005554, 0.2374766767024994, 0.31821209192276, 0.2586072087287903, 0.32279717922210693, 0.35883134603500366, 0.19203631579875946, 0.21960020065307617, 0.1921670138835907, 0.15228956937789917, 0.2805216610431671, 0.22011122107505798, 0.2743357717990875, 0.3582771420478821, 0.24900786578655243, 0.22482648491859436, 0.26258596777915955, 0.2373811900615692, 0.20743663609027863, 0.33088919520378113, 0.2164611518383026, 0.26570063829421997, 0.4280678629875183, 0.3759807348251343, 0.14579102396965027, 0.20970144867897034, 0.23536235094070435, 0.19280685484409332, 0.14028200507164001, 0.40082305669784546, 0.1878061592578888, 0.15089461207389832, 0.1855463683605194, 0.1860499233007431, 0.20608742535114288, 0.21330051124095917, 0.2762880027294159, 0.2870377004146576, 0.23827624320983887, 0.13925573229789734, 0.21858805418014526, 0.14608065783977509, 0.2204321175813675, 0.16657795011997223, 0.12870393693447113, 0.29310935735702515, 0.23631764948368073, 0.2712862491607666, 0.17068973183631897, 0.10661526024341583, 0.15675078332424164, 0.31675562262535095, 0.09855970740318298, 0.31195905804634094, 0.36966919898986816, 0.1652020364999771, 0.14555515348911285, 0.04613976180553436, 0.1910659223794937, 0.22908571362495422, 0.19769108295440674, 0.18489092588424683, 0.2685053050518036, 0.20844802260398865, 0.10465477406978607, 0.09163981676101685, 0.24475404620170593, 0.13480211794376373, 0.2387571781873703, 0.1634376049041748, 0.34268856048583984, 0.3357529044151306, 0.10940322279930115, 0.16674847900867462, 0.214692622423172, 0.23738227784633636, 0.32837679982185364, 0.2329413741827011, 0.14347021281719208, 0.1471080183982849, 0.2988130450248718, 0.22634555399417877, 0.25353294610977173, 0.13054707646369934, 0.12165665626525879, 0.1666906625032425, 0.28638955950737, 0.10700245946645737, 0.2595743238925934, 0.2648223638534546, 0.2674016058444977, 0.22810831665992737, 0.1404111087322235, 0.12549343705177307, 0.32314732670783997, 0.18129117786884308, 0.17536044120788574, 0.16549676656723022, 0.1350933313369751, 0.30411243438720703, 0.2887328267097473, 0.2266310751438141, 0.22999221086502075, 0.18519152700901031, 0.23129966855049133, 0.162003293633461, 0.2914866507053375, 0.1556156724691391, 0.1868385672569275, 0.21685510873794556, 0.17903733253479004, 0.15408645570278168, 0.10256125032901764, 0.25383424758911133, 0.12013676762580872, 0.15730299055576324, 0.25818946957588196, 0.20262570679187775, 0.17773430049419403, 0.2458076775074005, 0.20454953610897064, 0.13799330592155457, 0.2234274446964264, 0.1720626801252365, 0.16651473939418793, 0.19394785165786743, 0.18347220122814178, 0.24979403614997864, 0.23143641650676727, 0.09269649535417557, 0.26882970333099365, 0.23790448904037476, 0.15271952748298645, 0.14627155661582947, 0.2384713590145111, 0.14192485809326172, 0.1392241269350052, 0.22845283150672913, 0.09817938506603241, 0.09689144045114517, 0.10817506909370422, 0.12174463272094727, 0.1753820925951004, 0.19027334451675415, 0.191606342792511, 0.28711891174316406, 0.3069135844707489, 0.2561209201812744, 0.11602839082479477, 0.21851475536823273, 0.14067772030830383, 0.3284119665622711, 0.1266883760690689, 0.2840276062488556, 0.2378094494342804, 0.11570841073989868, 0.1727467030286789, 0.23531585931777954, 0.2886374294757843, 0.42380741238594055, 0.20320703089237213, 0.22758471965789795, 0.2148124873638153, 0.1734204888343811, 0.16654200851917267, 0.2830348610877991, 0.1846698820590973, 0.2645432949066162, 0.18453124165534973, 0.23723316192626953, 0.22855383157730103, 0.151118203997612, 0.1787903755903244, 0.19697190821170807, 0.18165023624897003, 0.1089906394481659, 0.2042919099330902, 0.1956566423177719, 0.1767861396074295, 0.25481265783309937, 0.18313249945640564, 0.17600040137767792, 0.24444347620010376, 0.12603698670864105, 0.2586425840854645, 0.27029332518577576, 0.18247513473033905, 0.2813439667224884, 0.2424364984035492, 0.20863984525203705, 0.19767677783966064, 0.20392484962940216, 0.1663009226322174, 0.24296224117279053, 0.3828870356082916, 0.18081262707710266, 0.2045125663280487, 0.260037899017334, 0.3135077655315399, 0.39472779631614685, 0.24057400226593018, 0.17522531747817993, 0.19273731112480164, 0.2400103062391281, 0.43738025426864624, 0.47785717248916626, 0.3658142387866974, 0.3975551724433899, 0.4051794707775116, 0.13488930463790894, 0.2903331518173218, 0.22873277962207794, 0.13597562909126282, 0.3258278965950012, 0.21590320765972137, 0.28973865509033203, 0.23034143447875977, 0.23181962966918945, 0.3031972050666809, 0.3500003516674042, 0.25935032963752747, 0.2556723654270172, 0.3080419600009918, 0.39690497517585754, 0.19395208358764648, 0.2694675922393799, 0.1601657271385193, 0.32112568616867065, 0.24649874866008759, 0.3543557822704315, 0.2086811065673828, 0.2066023200750351, 0.2990466356277466, 0.21009494364261627, 0.1510203778743744, 0.23054124414920807, 0.23972007632255554, 0.18253673613071442, 0.2000773698091507, 0.1606997698545456, 0.2739707827568054]\n",
      "\n",
      "Epoch [6/10], Step [10/274], Loss: 0.1301\n",
      "Epoch [6/10], Step [20/274], Loss: 0.1909\n",
      "Epoch [6/10], Step [30/274], Loss: 0.2811\n",
      "Epoch [6/10], Step [40/274], Loss: 0.1379\n",
      "Epoch [6/10], Step [50/274], Loss: 0.1509\n",
      "Epoch [6/10], Step [60/274], Loss: 0.0634\n",
      "Epoch [6/10], Step [70/274], Loss: 0.1071\n",
      "Epoch [6/10], Step [80/274], Loss: 0.1754\n",
      "Epoch [6/10], Step [90/274], Loss: 0.2079\n",
      "Epoch [6/10], Step [100/274], Loss: 0.1142\n",
      "Epoch [6/10], Step [110/274], Loss: 0.2286\n",
      "Epoch [6/10], Step [120/274], Loss: 0.2284\n",
      "Epoch [6/10], Step [130/274], Loss: 0.1452\n",
      "Epoch [6/10], Step [140/274], Loss: 0.1226\n",
      "Epoch [6/10], Step [150/274], Loss: 0.0986\n",
      "Epoch [6/10], Step [160/274], Loss: 0.2423\n",
      "Epoch [6/10], Step [170/274], Loss: 0.3478\n",
      "Epoch [6/10], Step [180/274], Loss: 0.1861\n",
      "Epoch [6/10], Step [190/274], Loss: 0.1950\n",
      "Epoch [6/10], Step [200/274], Loss: 0.2022\n",
      "Epoch [6/10], Step [210/274], Loss: 0.1856\n",
      "Epoch [6/10], Step [220/274], Loss: 0.1035\n",
      "Epoch [6/10], Step [230/274], Loss: 0.1786\n",
      "Epoch [6/10], Step [240/274], Loss: 0.1590\n",
      "Epoch [6/10], Step [250/274], Loss: 0.2289\n",
      "Epoch [6/10], Step [260/274], Loss: 0.1473\n",
      "Epoch [6/10], Step [270/274], Loss: 0.1086\n",
      "total_loss = [0.13015355169773102, 0.12148924171924591, 0.11092351377010345, 0.14721329510211945, 0.1741274893283844, 0.1047571450471878, 0.15875951945781708, 0.1853460669517517, 0.16600728034973145, 0.13011512160301208, 0.056415118277072906, 0.26091715693473816, 0.13265785574913025, 0.10574258118867874, 0.18340712785720825, 0.2153708040714264, 0.18144677579402924, 0.16209524869918823, 0.16301943361759186, 0.19092713296413422, 0.190202534198761, 0.09298324584960938, 0.1212269514799118, 0.12209932506084442, 0.2106739729642868, 0.20891574025154114, 0.1588917374610901, 0.08562448620796204, 0.15483492612838745, 0.2810749113559723, 0.11574004590511322, 0.06176113337278366, 0.12721344828605652, 0.14228332042694092, 0.09245452284812927, 0.15060490369796753, 0.12621046602725983, 0.07959363609552383, 0.2628299593925476, 0.13794061541557312, 0.22877627611160278, 0.20335868000984192, 0.10085983574390411, 0.18115465342998505, 0.08298011124134064, 0.21529975533485413, 0.1875341534614563, 0.23864269256591797, 0.17958328127861023, 0.15091201663017273, 0.20530539751052856, 0.12251689285039902, 0.16122883558273315, 0.17665284872055054, 0.14197339117527008, 0.15986859798431396, 0.1844247728586197, 0.24610163271427155, 0.11423195153474808, 0.06341588497161865, 0.08383682370185852, 0.10327371954917908, 0.11866629123687744, 0.17056047916412354, 0.14774593710899353, 0.24015474319458008, 0.05068304389715195, 0.11844924092292786, 0.24954108893871307, 0.10714615136384964, 0.1717008501291275, 0.3662608563899994, 0.1640542894601822, 0.06273124366998672, 0.08702554553747177, 0.16305217146873474, 0.19059745967388153, 0.19063973426818848, 0.10261952877044678, 0.17542997002601624, 0.17877863347530365, 0.2842659652233124, 0.17094211280345917, 0.13090680539608002, 0.18590936064720154, 0.13399213552474976, 0.15480279922485352, 0.10586144030094147, 0.16292305290699005, 0.20793160796165466, 0.07091690599918365, 0.18824569880962372, 0.15099762380123138, 0.11756006628274918, 0.08753614127635956, 0.09214708209037781, 0.1741357296705246, 0.08976772427558899, 0.0898418053984642, 0.11415114998817444, 0.30630308389663696, 0.17845851182937622, 0.21356572210788727, 0.10280914604663849, 0.18046586215496063, 0.19467519223690033, 0.23043805360794067, 0.16016504168510437, 0.16035625338554382, 0.22860530018806458, 0.22338074445724487, 0.17276141047477722, 0.0755636915564537, 0.17966800928115845, 0.23496246337890625, 0.14892420172691345, 0.1838836371898651, 0.1441977173089981, 0.24262726306915283, 0.22839927673339844, 0.12395595014095306, 0.3466310501098633, 0.13139352202415466, 0.13498064875602722, 0.24712146818637848, 0.15948382019996643, 0.22189435362815857, 0.12027142941951752, 0.2586382031440735, 0.14520376920700073, 0.1480989307165146, 0.12993553280830383, 0.20162411034107208, 0.07462478429079056, 0.0698712021112442, 0.06563776731491089, 0.19068041443824768, 0.1174897849559784, 0.059966444969177246, 0.12257895618677139, 0.15781396627426147, 0.19796408712863922, 0.16449673473834991, 0.20329827070236206, 0.16338567435741425, 0.20234252512454987, 0.15257687866687775, 0.26777946949005127, 0.20770297944545746, 0.09855548292398453, 0.13158512115478516, 0.2366170734167099, 0.09129567444324493, 0.11822308599948883, 0.0913565531373024, 0.3269677758216858, 0.10886679589748383, 0.16333535313606262, 0.21338751912117004, 0.24231401085853577, 0.06013096868991852, 0.2731900215148926, 0.1826515942811966, 0.11611823737621307, 0.18249958753585815, 0.24978239834308624, 0.1484917551279068, 0.16545534133911133, 0.14241047203540802, 0.34784308075904846, 0.23219235241413116, 0.12806712090969086, 0.11464966833591461, 0.17391958832740784, 0.12001898884773254, 0.23476363718509674, 0.23462621867656708, 0.1799667328596115, 0.2520042359828949, 0.18606676161289215, 0.25856518745422363, 0.15758666396141052, 0.12157679349184036, 0.23512965440750122, 0.15626581013202667, 0.15093442797660828, 0.14080744981765747, 0.16190528869628906, 0.20214976370334625, 0.19504870474338531, 0.19201111793518066, 0.11741451174020767, 0.10357946157455444, 0.19231724739074707, 0.20815034210681915, 0.1641416847705841, 0.1477033495903015, 0.1445002257823944, 0.18299540877342224, 0.20215089619159698, 0.07456322014331818, 0.08579076826572418, 0.2130875289440155, 0.09008155018091202, 0.12482447922229767, 0.16833943128585815, 0.15084308385849, 0.1414998173713684, 0.07023216784000397, 0.1855994164943695, 0.10930347442626953, 0.17041730880737305, 0.24741344153881073, 0.13098366558551788, 0.22283302247524261, 0.27298998832702637, 0.17627674341201782, 0.217877596616745, 0.3133380115032196, 0.10352154076099396, 0.21542711555957794, 0.15454283356666565, 0.15736554563045502, 0.10235225409269333, 0.09918272495269775, 0.23687432706356049, 0.32070791721343994, 0.34094417095184326, 0.10274701565504074, 0.1786421537399292, 0.15722660720348358, 0.17366114258766174, 0.4709298014640808, 0.627825915813446, 0.4693705141544342, 0.2334427535533905, 0.24793623387813568, 0.0943763256072998, 0.2878645360469818, 0.15903998911380768, 0.30023401975631714, 0.1787947416305542, 0.23103633522987366, 0.13914385437965393, 0.2029082030057907, 0.19481909275054932, 0.20991156995296478, 0.1383679211139679, 0.2563285827636719, 0.2289155274629593, 0.19894735515117645, 0.298143595457077, 0.24801000952720642, 0.19495494663715363, 0.14956921339035034, 0.3017086684703827, 0.21717716753482819, 0.18687081336975098, 0.1830199658870697, 0.1472528874874115, 0.2966911792755127, 0.27579107880592346, 0.17112372815608978, 0.10879148542881012, 0.15444546937942505, 0.1347057819366455, 0.3059433102607727, 0.20532916486263275, 0.12494753301143646, 0.10862267017364502, 0.17436015605926514, 0.12038331478834152, 0.14597277343273163, 0.2349783480167389]\n",
      "\n",
      "Epoch [7/10], Step [10/274], Loss: 0.0493\n",
      "Epoch [7/10], Step [20/274], Loss: 0.1451\n",
      "Epoch [7/10], Step [30/274], Loss: 0.1929\n",
      "Epoch [7/10], Step [40/274], Loss: 0.0558\n",
      "Epoch [7/10], Step [50/274], Loss: 0.2357\n",
      "Epoch [7/10], Step [60/274], Loss: 0.1355\n",
      "Epoch [7/10], Step [70/274], Loss: 0.1258\n",
      "Epoch [7/10], Step [80/274], Loss: 0.2961\n",
      "Epoch [7/10], Step [90/274], Loss: 0.0616\n",
      "Epoch [7/10], Step [100/274], Loss: 0.2494\n",
      "Epoch [7/10], Step [110/274], Loss: 0.1108\n",
      "Epoch [7/10], Step [120/274], Loss: 0.1481\n",
      "Epoch [7/10], Step [130/274], Loss: 0.1054\n",
      "Epoch [7/10], Step [140/274], Loss: 0.0525\n",
      "Epoch [7/10], Step [150/274], Loss: 0.0926\n",
      "Epoch [7/10], Step [160/274], Loss: 0.1184\n",
      "Epoch [7/10], Step [170/274], Loss: 0.0436\n",
      "Epoch [7/10], Step [180/274], Loss: 0.1321\n",
      "Epoch [7/10], Step [190/274], Loss: 0.0466\n",
      "Epoch [7/10], Step [200/274], Loss: 0.2173\n",
      "Epoch [7/10], Step [210/274], Loss: 0.0598\n",
      "Epoch [7/10], Step [220/274], Loss: 0.0676\n",
      "Epoch [7/10], Step [230/274], Loss: 0.1456\n",
      "Epoch [7/10], Step [240/274], Loss: 0.1229\n",
      "Epoch [7/10], Step [250/274], Loss: 0.0632\n",
      "Epoch [7/10], Step [260/274], Loss: 0.1218\n",
      "Epoch [7/10], Step [270/274], Loss: 0.1438\n",
      "total_loss = [0.10318273305892944, 0.03948032855987549, 0.12007129192352295, 0.1013413816690445, 0.10909409821033478, 0.054241739213466644, 0.06939532607793808, 0.1280306577682495, 0.06403325498104095, 0.049305252730846405, 0.057377398014068604, 0.10254000127315521, 0.1793111115694046, 0.16891220211982727, 0.08763804286718369, 0.05734260752797127, 0.1444961428642273, 0.12965720891952515, 0.05570662394165993, 0.14507009088993073, 0.11140652000904083, 0.20251885056495667, 0.04597841203212738, 0.19568327069282532, 0.14687713980674744, 0.11623373627662659, 0.10456407070159912, 0.1413668394088745, 0.22775068879127502, 0.19288519024848938, 0.13031019270420074, 0.1362505555152893, 0.22779688239097595, 0.09017059206962585, 0.24170249700546265, 0.35019558668136597, 0.13186120986938477, 0.31342846155166626, 0.1874079704284668, 0.055774565786123276, 0.1645345538854599, 0.10866115242242813, 0.22722281515598297, 0.2583741545677185, 0.22417591512203217, 0.051503002643585205, 0.24760979413986206, 0.09204155951738358, 0.09850087016820908, 0.23568375408649445, 0.1497662216424942, 0.10648826509714127, 0.09684198349714279, 0.12867340445518494, 0.12876397371292114, 0.2287716120481491, 0.1357303112745285, 0.11483799666166306, 0.11953867971897125, 0.1355355829000473, 0.2309550940990448, 0.13286538422107697, 0.2032889723777771, 0.10551033914089203, 0.15798890590667725, 0.12624242901802063, 0.06990162283182144, 0.08069837093353271, 0.13811905682086945, 0.12577137351036072, 0.13772976398468018, 0.09117409586906433, 0.09235969185829163, 0.09324613213539124, 0.0784984603524208, 0.07160952687263489, 0.08930961787700653, 0.110800601541996, 0.11340939253568649, 0.2961438298225403, 0.13905620574951172, 0.0919717326760292, 0.14846500754356384, 0.09221542626619339, 0.2068192958831787, 0.10070066154003143, 0.056717399507761, 0.06324128061532974, 0.1321977972984314, 0.0616031214594841, 0.21398566663265228, 0.07535545527935028, 0.1463117152452469, 0.09837861359119415, 0.10747502744197845, 0.0987037643790245, 0.16942089796066284, 0.07180190086364746, 0.12684297561645508, 0.2493779957294464, 0.11394235491752625, 0.15886172652244568, 0.17672133445739746, 0.1261795461177826, 0.166850283741951, 0.14659908413887024, 0.18799293041229248, 0.13485418260097504, 0.11974984407424927, 0.11078318953514099, 0.14832042157649994, 0.2616870105266571, 0.259182870388031, 0.11565720289945602, 0.17352518439292908, 0.22266696393489838, 0.10927273333072662, 0.06865105032920837, 0.16083131730556488, 0.14810505509376526, 0.16141963005065918, 0.2345586121082306, 0.14101435244083405, 0.0711049884557724, 0.06303245574235916, 0.0791626051068306, 0.17160500586032867, 0.17445288598537445, 0.13565658032894135, 0.10542108118534088, 0.08436475694179535, 0.12088410556316376, 0.09910737723112106, 0.09319299459457397, 0.16502968966960907, 0.10105204582214355, 0.1140582412481308, 0.106266088783741, 0.2310831993818283, 0.052541811019182205, 0.05182637274265289, 0.20277039706707, 0.08345507830381393, 0.2897171974182129, 0.03146536648273468, 0.13607755303382874, 0.14246642589569092, 0.20593076944351196, 0.12178855389356613, 0.0925627276301384, 0.052854087203741074, 0.12650726735591888, 0.10455696284770966, 0.12897643446922302, 0.10604290664196014, 0.03734399750828743, 0.07248968631029129, 0.20353320240974426, 0.08019743859767914, 0.11840513348579407, 0.16194476187229156, 0.07412135601043701, 0.08135578036308289, 0.10749822109937668, 0.08890768885612488, 0.1850185990333557, 0.23826178908348083, 0.1959589272737503, 0.036778129637241364, 0.043555233627557755, 0.16181224584579468, 0.11752572655677795, 0.15232761204242706, 0.09208829700946808, 0.05196870490908623, 0.13539741933345795, 0.20490100979804993, 0.21073392033576965, 0.039016325026750565, 0.13209596276283264, 0.25710150599479675, 0.20058923959732056, 0.4662927985191345, 0.2562534511089325, 0.17476330697536469, 0.028107110410928726, 0.14997240900993347, 0.09731754660606384, 0.07613454014062881, 0.04655519127845764, 0.08404569327831268, 0.11975884437561035, 0.13965927064418793, 0.16098235547542572, 0.09099087864160538, 0.16841256618499756, 0.16383041441440582, 0.05440479889512062, 0.06140526756644249, 0.217254638671875, 0.12367881834506989, 0.12976346909999847, 0.0411563366651535, 0.2564440071582794, 0.1966371238231659, 0.13932672142982483, 0.03161850571632385, 0.06842998415231705, 0.28791382908821106, 0.059792742133140564, 0.12883663177490234, 0.18026003241539001, 0.09555728733539581, 0.142776221036911, 0.15509799122810364, 0.07325579971075058, 0.1833896040916443, 0.25212424993515015, 0.15607494115829468, 0.06762400269508362, 0.1251475214958191, 0.13417279720306396, 0.18842269480228424, 0.10500442236661911, 0.24153724312782288, 0.20956265926361084, 0.22407078742980957, 0.23323898017406464, 0.06531555950641632, 0.14563234150409698, 0.1409413367509842, 0.08166676759719849, 0.2943446934223175, 0.04734095185995102, 0.1454995721578598, 0.2504488229751587, 0.06489434838294983, 0.18218302726745605, 0.21473661065101624, 0.12286370992660522, 0.18598856031894684, 0.11810505390167236, 0.08453861624002457, 0.16396045684814453, 0.09363498538732529, 0.21863171458244324, 0.20427729189395905, 0.0641942098736763, 0.07675012946128845, 0.06318549811840057, 0.07791321724653244, 0.06809372454881668, 0.1632767617702484, 0.22619374096393585, 0.07102765142917633, 0.0354648157954216, 0.07558079808950424, 0.1602151244878769, 0.1073005273938179, 0.12184213101863861, 0.1633094847202301, 0.25856947898864746, 0.12409426271915436, 0.2815704345703125, 0.1585570126771927, 0.09755180031061172, 0.14099210500717163, 0.10988987982273102, 0.36088958382606506, 0.14375826716423035, 0.3070354461669922, 0.24010883271694183, 0.07615208625793457, 0.07497856020927429]\n",
      "\n",
      "Epoch [8/10], Step [10/274], Loss: 0.1257\n",
      "Epoch [8/10], Step [20/274], Loss: 0.0653\n",
      "Epoch [8/10], Step [30/274], Loss: 0.1214\n",
      "Epoch [8/10], Step [40/274], Loss: 0.0991\n",
      "Epoch [8/10], Step [50/274], Loss: 0.1395\n",
      "Epoch [8/10], Step [60/274], Loss: 0.1867\n",
      "Epoch [8/10], Step [70/274], Loss: 0.0654\n",
      "Epoch [8/10], Step [80/274], Loss: 0.0407\n",
      "Epoch [8/10], Step [90/274], Loss: 0.1875\n",
      "Epoch [8/10], Step [100/274], Loss: 0.0884\n",
      "Epoch [8/10], Step [110/274], Loss: 0.0582\n",
      "Epoch [8/10], Step [120/274], Loss: 0.0325\n",
      "Epoch [8/10], Step [130/274], Loss: 0.0788\n",
      "Epoch [8/10], Step [140/274], Loss: 0.0241\n",
      "Epoch [8/10], Step [150/274], Loss: 0.0630\n",
      "Epoch [8/10], Step [160/274], Loss: 0.0804\n",
      "Epoch [8/10], Step [170/274], Loss: 0.0459\n",
      "Epoch [8/10], Step [180/274], Loss: 0.0832\n",
      "Epoch [8/10], Step [190/274], Loss: 0.0276\n",
      "Epoch [8/10], Step [200/274], Loss: 0.1594\n",
      "Epoch [8/10], Step [210/274], Loss: 0.1087\n",
      "Epoch [8/10], Step [220/274], Loss: 0.1116\n",
      "Epoch [8/10], Step [230/274], Loss: 0.0993\n",
      "Epoch [8/10], Step [240/274], Loss: 0.0504\n",
      "Epoch [8/10], Step [250/274], Loss: 0.0779\n",
      "Epoch [8/10], Step [260/274], Loss: 0.2347\n",
      "Epoch [8/10], Step [270/274], Loss: 0.0975\n",
      "total_loss = [0.07508209347724915, 0.04474068805575371, 0.22683098912239075, 0.19965721666812897, 0.18716716766357422, 0.4916328191757202, 0.2232949137687683, 0.13848890364170074, 0.0759839117527008, 0.1256590038537979, 0.13649588823318481, 0.12664006650447845, 0.1459435224533081, 0.15603621304035187, 0.17984405159950256, 0.05246764421463013, 0.08895468711853027, 0.08732478320598602, 0.03335603326559067, 0.06530217081308365, 0.10179276764392853, 0.05797099322080612, 0.11615698039531708, 0.058270163834095, 0.1087876409292221, 0.06230258569121361, 0.18914984166622162, 0.13253721594810486, 0.06513859331607819, 0.12137149274349213, 0.06353512406349182, 0.054829150438308716, 0.06107237935066223, 0.08537755161523819, 0.09585601091384888, 0.015037951059639454, 0.06077496334910393, 0.10530410706996918, 0.01646057516336441, 0.09909739345312119, 0.06328704208135605, 0.09540269523859024, 0.19311636686325073, 0.042621493339538574, 0.18347890675067902, 0.08153907209634781, 0.06817969679832458, 0.16254417598247528, 0.09990070015192032, 0.139474555850029, 0.0834178626537323, 0.03057783469557762, 0.04456460475921631, 0.06370925903320312, 0.08293555676937103, 0.13114453852176666, 0.08469825983047485, 0.09708600491285324, 0.04746624827384949, 0.18667127192020416, 0.04357767477631569, 0.1380874514579773, 0.04476384073495865, 0.04081336036324501, 0.054506320506334305, 0.05201307684183121, 0.11975337564945221, 0.21540918946266174, 0.03589106351137161, 0.0653885155916214, 0.06818437576293945, 0.14297637343406677, 0.12701964378356934, 0.07796277850866318, 0.03294507786631584, 0.02384624443948269, 0.08893425762653351, 0.0346718430519104, 0.04459081217646599, 0.040679749101400375, 0.021195625886321068, 0.23892800509929657, 0.049281783401966095, 0.0712096095085144, 0.08968052268028259, 0.12531724572181702, 0.03238077461719513, 0.07470838725566864, 0.048059556633234024, 0.18749603629112244, 0.056123025715351105, 0.12828963994979858, 0.18529507517814636, 0.042295437306165695, 0.09278383105993271, 0.190608412027359, 0.05533921718597412, 0.10468000173568726, 0.07290995866060257, 0.08840031921863556, 0.07747089862823486, 0.06366653740406036, 0.06342767924070358, 0.19284191727638245, 0.11962669342756271, 0.2721361815929413, 0.11683162301778793, 0.1460825800895691, 0.11921978741884232, 0.058157406747341156, 0.11809027194976807, 0.03767320141196251, 0.045432351529598236, 0.19416874647140503, 0.03658074140548706, 0.0801202803850174, 0.07811333239078522, 0.06018860638141632, 0.14288923144340515, 0.032462600618600845, 0.33372241258621216, 0.05499572306871414, 0.07162071764469147, 0.06117464229464531, 0.09612371027469635, 0.12400323897600174, 0.08126512169837952, 0.027459371834993362, 0.061633337289094925, 0.07879041135311127, 0.04756464809179306, 0.1487066149711609, 0.05664726346731186, 0.14090195298194885, 0.07917823642492294, 0.024682044982910156, 0.05277997627854347, 0.03218161687254906, 0.06179923564195633, 0.024093277752399445, 0.010553283616900444, 0.11278572678565979, 0.025658834725618362, 0.019442584365606308, 0.032822590321302414, 0.12644581496715546, 0.02655158005654812, 0.009447804652154446, 0.013856721110641956, 0.06298138201236725, 0.12202535569667816, 0.06955382227897644, 0.08682303130626678, 0.04101298004388809, 0.09494467079639435, 0.08635478466749191, 0.16221004724502563, 0.2400117665529251, 0.26562410593032837, 0.08036080002784729, 0.12094537168741226, 0.06979557871818542, 0.07606333494186401, 0.07348272204399109, 0.09994293004274368, 0.06580319255590439, 0.12271344661712646, 0.18164148926734924, 0.11181250959634781, 0.04590320587158203, 0.04941107705235481, 0.05116462707519531, 0.05893922969698906, 0.04793801158666611, 0.04318266734480858, 0.12568256258964539, 0.16249114274978638, 0.02997230365872383, 0.0617976039648056, 0.0832335352897644, 0.1585497409105301, 0.13950568437576294, 0.3070629835128784, 0.1344008445739746, 0.05105871334671974, 0.32698458433151245, 0.07845218479633331, 0.08544035255908966, 0.05015263706445694, 0.027616087347269058, 0.02069208398461342, 0.10951235890388489, 0.09048174321651459, 0.07419461011886597, 0.028264284133911133, 0.12863656878471375, 0.14676490426063538, 0.08532164990901947, 0.03476434201002121, 0.15936122834682465, 0.04916558414697647, 0.11891379952430725, 0.20457199215888977, 0.049411043524742126, 0.0849359929561615, 0.08140619844198227, 0.05449056252837181, 0.16818761825561523, 0.06857580691576004, 0.1086883395910263, 0.06625272333621979, 0.13399918377399445, 0.08912450075149536, 0.08589395880699158, 0.05051533132791519, 0.10744035243988037, 0.026008013635873795, 0.11488139629364014, 0.06186269596219063, 0.11155557632446289, 0.03786464035511017, 0.2743788957595825, 0.17288506031036377, 0.05030636861920357, 0.11263062059879303, 0.03801153227686882, 0.04445299506187439, 0.059166405349969864, 0.03836742416024208, 0.0993463397026062, 0.07801198214292526, 0.0663941353559494, 0.08431390672922134, 0.022281749173998833, 0.05670735985040665, 0.040066782385110855, 0.08994408696889877, 0.02207176387310028, 0.04647098854184151, 0.050396524369716644, 0.2060626745223999, 0.007494415622204542, 0.03574065864086151, 0.08228906989097595, 0.1381874680519104, 0.25724560022354126, 0.12149226665496826, 0.19517160952091217, 0.303860604763031, 0.07794748246669769, 0.1583658754825592, 0.06668069958686829, 0.09009817242622375, 0.17230363190174103, 0.20831574499607086, 0.04273306950926781, 0.03198261559009552, 0.04748149961233139, 0.08471119403839111, 0.23465827107429504, 0.033353451639413834, 0.08545734733343124, 0.07453936338424683, 0.17606157064437866, 0.12203400582075119, 0.04119855910539627, 0.07785255461931229, 0.19892774522304535, 0.09329535812139511, 0.09745801985263824, 0.05663536489009857, 0.0903717428445816, 0.09200280159711838, 0.05011528357863426]\n",
      "\n",
      "Epoch [9/10], Step [10/274], Loss: 0.3065\n",
      "Epoch [9/10], Step [20/274], Loss: 0.0538\n",
      "Epoch [9/10], Step [30/274], Loss: 0.0250\n",
      "Epoch [9/10], Step [40/274], Loss: 0.0170\n",
      "Epoch [9/10], Step [50/274], Loss: 0.0239\n",
      "Epoch [9/10], Step [60/274], Loss: 0.0523\n",
      "Epoch [9/10], Step [70/274], Loss: 0.0109\n",
      "Epoch [9/10], Step [80/274], Loss: 0.0359\n",
      "Epoch [9/10], Step [90/274], Loss: 0.0143\n",
      "Epoch [9/10], Step [100/274], Loss: 0.0821\n",
      "Epoch [9/10], Step [110/274], Loss: 0.1017\n",
      "Epoch [9/10], Step [120/274], Loss: 0.0377\n",
      "Epoch [9/10], Step [130/274], Loss: 0.1671\n",
      "Epoch [9/10], Step [140/274], Loss: 0.0991\n",
      "Epoch [9/10], Step [150/274], Loss: 0.1746\n",
      "Epoch [9/10], Step [160/274], Loss: 0.0158\n",
      "Epoch [9/10], Step [170/274], Loss: 0.1507\n",
      "Epoch [9/10], Step [180/274], Loss: 0.0305\n",
      "Epoch [9/10], Step [190/274], Loss: 0.0895\n",
      "Epoch [9/10], Step [200/274], Loss: 0.1142\n",
      "Epoch [9/10], Step [210/274], Loss: 0.0741\n",
      "Epoch [9/10], Step [220/274], Loss: 0.0301\n",
      "Epoch [9/10], Step [230/274], Loss: 0.1048\n",
      "Epoch [9/10], Step [240/274], Loss: 0.1454\n",
      "Epoch [9/10], Step [250/274], Loss: 0.0378\n",
      "Epoch [9/10], Step [260/274], Loss: 0.0440\n",
      "Epoch [9/10], Step [270/274], Loss: 0.0672\n",
      "total_loss = [0.07850049436092377, 0.07112987339496613, 0.018063584342598915, 0.04932289570569992, 0.06188420206308365, 0.04897920787334442, 0.10183033347129822, 0.09020353853702545, 0.15883325040340424, 0.30650612711906433, 0.021859286352992058, 0.08187641203403473, 0.063670314848423, 0.0779920294880867, 0.06580211222171783, 0.12626595795154572, 0.06232177093625069, 0.11043435335159302, 0.07037026435136795, 0.053772784769535065, 0.025542177259922028, 0.043502017855644226, 0.07472436130046844, 0.04478014260530472, 0.044673141092061996, 0.13871222734451294, 0.08804840594530106, 0.14264565706253052, 0.0443425215780735, 0.024957599118351936, 0.045920707285404205, 0.04273171350359917, 0.050157103687524796, 0.023775316774845123, 0.057671092450618744, 0.09701381623744965, 0.036015402525663376, 0.09664510190486908, 0.11827976256608963, 0.016957590356469154, 0.016994651407003403, 0.07426708936691284, 0.04651625454425812, 0.0228513702750206, 0.1000489741563797, 0.018072694540023804, 0.058036919683218, 0.03542333096265793, 0.05082431435585022, 0.023851901292800903, 0.06797170639038086, 0.021361328661441803, 0.05088372901082039, 0.05195176601409912, 0.02374878153204918, 0.034173958003520966, 0.08780139684677124, 0.019747048616409302, 0.13833805918693542, 0.05227123945951462, 0.023991627618670464, 0.023643523454666138, 0.1383015513420105, 0.07161201536655426, 0.04190210998058319, 0.032912518829107285, 0.022435100749135017, 0.022762611508369446, 0.019970009103417397, 0.010905646719038486, 0.009126590564846992, 0.025665460154414177, 0.09849902987480164, 0.062187712639570236, 0.03913690149784088, 0.054880257695913315, 0.10858087986707687, 0.01301928423345089, 0.11597631871700287, 0.03585818409919739, 0.06348899006843567, 0.006829274818301201, 0.0234257522970438, 0.031140422448515892, 0.009226333349943161, 0.042017657309770584, 0.028711842373013496, 0.012115265242755413, 0.1412244588136673, 0.014261643402278423, 0.09665060043334961, 0.03375229239463806, 0.020200729370117188, 0.01836390234529972, 0.17957721650600433, 0.08066949993371964, 0.055128224194049835, 0.16964685916900635, 0.08345215022563934, 0.0820513516664505, 0.06026868522167206, 0.06957410275936127, 0.10096520185470581, 0.07118309289216995, 0.21058818697929382, 0.11320248991250992, 0.02981145866215229, 0.06317326426506042, 0.037803251296281815, 0.10173708945512772, 0.05511569231748581, 0.03631768375635147, 0.028310177847743034, 0.07615239918231964, 0.07034152746200562, 0.06781743466854095, 0.04539964720606804, 0.026802226901054382, 0.09459557384252548, 0.03765455633401871, 0.027790024876594543, 0.07020330429077148, 0.032013826072216034, 0.013538600876927376, 0.01671612448990345, 0.04078313708305359, 0.011684810742735863, 0.012749547138810158, 0.07465493679046631, 0.16707827150821686, 0.09017938375473022, 0.19783654808998108, 0.2199425846338272, 0.11713641881942749, 0.09461358189582825, 0.12529447674751282, 0.20258331298828125, 0.13397163152694702, 0.1994827687740326, 0.0991399735212326, 0.17261415719985962, 0.04591231793165207, 0.08091318607330322, 0.0936727225780487, 0.14791339635849, 0.167744442820549, 0.17160369455814362, 0.11078159511089325, 0.1635383665561676, 0.17458611726760864, 0.07142540067434311, 0.09278471767902374, 0.14713513851165771, 0.12059111893177032, 0.07408393919467926, 0.07317602634429932, 0.11118634045124054, 0.19410920143127441, 0.05593376234173775, 0.01577705517411232, 0.06968536972999573, 0.10766206681728363, 0.0757179856300354, 0.07264125347137451, 0.12649324536323547, 0.015302736312150955, 0.08562421798706055, 0.05480774864554405, 0.07574349641799927, 0.1507282257080078, 0.021766170859336853, 0.05193060636520386, 0.1196882501244545, 0.027299314737319946, 0.05091758072376251, 0.03317226469516754, 0.023943457752466202, 0.07104314863681793, 0.1433725357055664, 0.030450014397501945, 0.01878722943365574, 0.09830653667449951, 0.02559901773929596, 0.0419485904276371, 0.020415140315890312, 0.025761771947145462, 0.08344664424657822, 0.016074005514383316, 0.08714646100997925, 0.08951301127672195, 0.04376184195280075, 0.08531951904296875, 0.02077558822929859, 0.05752363055944443, 0.009351365268230438, 0.04652605950832367, 0.020183466374874115, 0.014223907142877579, 0.05009905993938446, 0.11418291926383972, 0.06731449067592621, 0.11004684865474701, 0.03580351173877716, 0.04694632440805435, 0.1522321105003357, 0.09261515736579895, 0.01750173233449459, 0.02611437439918518, 0.12091659009456635, 0.07406751066446304, 0.05843932181596756, 0.108640655875206, 0.03310180455446243, 0.0632094144821167, 0.035396281629800797, 0.0330730564892292, 0.01678554341197014, 0.014930247329175472, 0.06269591301679611, 0.03006807714700699, 0.06086317077279091, 0.022960158064961433, 0.153591588139534, 0.15432500839233398, 0.028285127133131027, 0.01307786162942648, 0.1303137242794037, 0.08572883903980255, 0.13034719228744507, 0.1048031598329544, 0.061649173498153687, 0.058367542922496796, 0.07420430332422256, 0.04008422791957855, 0.11259925365447998, 0.010362543165683746, 0.03648338466882706, 0.06071953848004341, 0.05590372160077095, 0.14538481831550598, 0.043476253747940063, 0.07337303459644318, 0.14493873715400696, 0.05726819112896919, 0.07290239632129669, 0.022579623386263847, 0.0800836980342865, 0.13126760721206665, 0.053175367414951324, 0.037826407700777054, 0.1531587839126587, 0.1303919106721878, 0.07442700862884521, 0.14098720252513885, 0.04566153138875961, 0.027783619239926338, 0.06796658039093018, 0.04695448279380798, 0.12008213996887207, 0.044032588601112366, 0.026173913851380348, 0.050520651042461395, 0.08306753635406494, 0.11365889012813568, 0.058330848813056946, 0.07436588406562805, 0.10983582586050034, 0.06481501460075378, 0.0840250551700592, 0.06724198162555695, 0.06287533044815063, 0.012512519024312496, 0.08739106357097626, 0.16592052578926086]\n",
      "\n",
      "Epoch [10/10], Step [10/274], Loss: 0.0245\n",
      "Epoch [10/10], Step [20/274], Loss: 0.0304\n",
      "Epoch [10/10], Step [30/274], Loss: 0.1050\n",
      "Epoch [10/10], Step [40/274], Loss: 0.0169\n",
      "Epoch [10/10], Step [50/274], Loss: 0.0778\n",
      "Epoch [10/10], Step [60/274], Loss: 0.0181\n",
      "Epoch [10/10], Step [70/274], Loss: 0.0099\n",
      "Epoch [10/10], Step [80/274], Loss: 0.0356\n",
      "Epoch [10/10], Step [90/274], Loss: 0.0065\n",
      "Epoch [10/10], Step [100/274], Loss: 0.0791\n",
      "Epoch [10/10], Step [110/274], Loss: 0.0067\n",
      "Epoch [10/10], Step [120/274], Loss: 0.0591\n",
      "Epoch [10/10], Step [130/274], Loss: 0.0090\n",
      "Epoch [10/10], Step [140/274], Loss: 0.0200\n",
      "Epoch [10/10], Step [150/274], Loss: 0.0221\n",
      "Epoch [10/10], Step [160/274], Loss: 0.0437\n",
      "Epoch [10/10], Step [170/274], Loss: 0.0458\n",
      "Epoch [10/10], Step [180/274], Loss: 0.0352\n",
      "Epoch [10/10], Step [190/274], Loss: 0.0221\n",
      "Epoch [10/10], Step [200/274], Loss: 0.0140\n",
      "Epoch [10/10], Step [210/274], Loss: 0.1176\n",
      "Epoch [10/10], Step [220/274], Loss: 0.0690\n",
      "Epoch [10/10], Step [230/274], Loss: 0.0860\n",
      "Epoch [10/10], Step [240/274], Loss: 0.0160\n",
      "Epoch [10/10], Step [250/274], Loss: 0.0706\n",
      "Epoch [10/10], Step [260/274], Loss: 0.0204\n",
      "Epoch [10/10], Step [270/274], Loss: 0.0398\n",
      "total_loss = [0.023251617327332497, 0.03158582001924515, 0.014659768901765347, 0.034272097051143646, 0.02762707881629467, 0.011568260379135609, 0.03631265088915825, 0.05875964090228081, 0.03628765419125557, 0.02452760562300682, 0.03163221478462219, 0.02883107028901577, 0.06021147221326828, 0.02381063625216484, 0.036178264766931534, 0.02019781805574894, 0.10606132447719574, 0.04352254420518875, 0.04988214373588562, 0.030359523370862007, 0.008340788073837757, 0.011667722836136818, 0.03483013063669205, 0.033913470804691315, 0.05536491423845291, 0.019776374101638794, 0.06938828527927399, 0.016331126913428307, 0.01226134318858385, 0.10496346652507782, 0.01036468893289566, 0.0908995121717453, 0.02686157450079918, 0.00659615732729435, 0.0373140349984169, 0.014379434287548065, 0.023953517898917198, 0.006117840297520161, 0.03228132799267769, 0.016904739663004875, 0.011599922552704811, 0.056632764637470245, 0.06192353367805481, 0.028301816433668137, 0.0687754824757576, 0.005039595067501068, 0.0628766417503357, 0.015973709523677826, 0.03223087638616562, 0.07778617739677429, 0.04342446103692055, 0.03511867672204971, 0.0127518055960536, 0.07026340067386627, 0.013416368514299393, 0.028434790670871735, 0.049022216349840164, 0.07595597207546234, 0.021290283650159836, 0.018074847757816315, 0.02230554074048996, 0.13513070344924927, 0.03669581934809685, 0.007789984345436096, 0.060273848474025726, 0.028435541316866875, 0.06750676035881042, 0.03456965833902359, 0.012629478238523006, 0.009898113086819649, 0.006303245201706886, 0.03178531676530838, 0.01729149930179119, 0.007404637522995472, 0.0033598151057958603, 0.015013433992862701, 0.04507419839501381, 0.021999921649694443, 0.01566682755947113, 0.035599853843450546, 0.08688825368881226, 0.014654530212283134, 0.018457770347595215, 0.013668825849890709, 0.024801326915621758, 0.03703143447637558, 0.008891372010111809, 0.022536911070346832, 0.0034508276730775833, 0.0064847590401768684, 0.026043914258480072, 0.04575815796852112, 0.042937666177749634, 0.03839610517024994, 0.02902640402317047, 0.015529166907072067, 0.03000704199075699, 0.014953402802348137, 0.01589927449822426, 0.07912679016590118, 0.06550893187522888, 0.010283228941261768, 0.018313124775886536, 0.00667580496519804, 0.0547666996717453, 0.030581887811422348, 0.02683768793940544, 0.009865486063063145, 0.08952601253986359, 0.006743541918694973, 0.024531880393624306, 0.026310410350561142, 0.02230071648955345, 0.004843426868319511, 0.13127931952476501, 0.0024794135242700577, 0.023165276274085045, 0.03642547130584717, 0.029331471771001816, 0.059134744107723236, 0.0383145846426487, 0.048364829272031784, 0.09275959432125092, 0.07405851036310196, 0.04407430440187454, 0.0448329821228981, 0.022994767874479294, 0.18138925731182098, 0.028518782928586006, 0.009007483720779419, 0.1738351434469223, 0.06827391684055328, 0.039148543030023575, 0.012801699340343475, 0.031248919665813446, 0.055157579481601715, 0.13512980937957764, 0.004388934001326561, 0.01882840320467949, 0.019970886409282684, 0.018877992406487465, 0.05301898717880249, 0.026471609249711037, 0.0765104666352272, 0.09743593633174896, 0.0069176433607935905, 0.0485905185341835, 0.043824125081300735, 0.08606620132923126, 0.022109799087047577, 0.03970227390527725, 0.08788697421550751, 0.01797144114971161, 0.04020640626549721, 0.02273848094046116, 0.10986922681331635, 0.004037084057927132, 0.070036381483078, 0.03456245735287666, 0.043662142008543015, 0.09898081421852112, 0.07646175473928452, 0.028591850772500038, 0.009631369262933731, 0.07540477067232132, 0.014911494217813015, 0.02078210934996605, 0.06766769289970398, 0.033401474356651306, 0.04576949402689934, 0.012381656095385551, 0.17602097988128662, 0.0033937208354473114, 0.11767151206731796, 0.01868513599038124, 0.06442578136920929, 0.03881091624498367, 0.032973796129226685, 0.009917579591274261, 0.035189226269721985, 0.009061932563781738, 0.03670956566929817, 0.13601012527942657, 0.0132141700014472, 0.017745113000273705, 0.005039375275373459, 0.05591666325926781, 0.010189259424805641, 0.006722338497638702, 0.022050466388463974, 0.007903487421572208, 0.02939225733280182, 0.024465061724185944, 0.0067327627912163734, 0.030884508043527603, 0.01745685003697872, 0.04017903283238411, 0.06094413623213768, 0.05219870060682297, 0.013999942690134048, 0.025039076805114746, 0.003727041184902191, 0.014718745835125446, 0.02036155015230179, 0.03178369998931885, 0.00929104071110487, 0.09961160272359848, 0.042684406042099, 0.021732188761234283, 0.11761212348937988, 0.010210059583187103, 0.00316611398011446, 0.05661335587501526, 0.0032317107543349266, 0.13202697038650513, 0.09254646301269531, 0.01801498420536518, 0.04635470733046532, 0.04042559489607811, 0.0689750462770462, 0.02553178369998932, 0.04700562357902527, 0.04831846058368683, 0.01613715849816799, 0.00930796004831791, 0.04613245651125908, 0.03345305100083351, 0.009755315259099007, 0.019184628501534462, 0.08600405603647232, 0.016900736838579178, 0.02795920893549919, 0.03646949306130409, 0.10105665773153305, 0.03745251148939133, 0.0269148088991642, 0.037593528628349304, 0.04325137659907341, 0.05370904505252838, 0.01604996621608734, 0.018560724332928658, 0.028693795204162598, 0.05110514909029007, 0.014092082157731056, 0.05583992227911949, 0.005210496485233307, 0.006747431121766567, 0.14210137724876404, 0.005425155162811279, 0.0706377923488617, 0.026466937735676765, 0.04312368854880333, 0.04117804765701294, 0.08926025032997131, 0.01749618537724018, 0.03205745294690132, 0.034004151821136475, 0.015127090737223625, 0.05629841610789299, 0.020372789353132248, 0.07254725694656372, 0.07005985826253891, 0.06628192961215973, 0.03386084735393524, 0.03237446770071983, 0.04408133029937744, 0.016576699912548065, 0.021305708214640617, 0.06212785094976425, 0.03984618932008743, 0.010708991438150406, 0.025651095435023308, 0.00490507110953331, 0.007752574048936367]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "\"\"\"\n",
    "@author: Yantong Lai\n",
    "@date: 2019.11.9\n",
    "\"\"\"\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext import data, datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"../data/aclImdb/\"\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "####################################\n",
    "#         Hyper-parameters         #\n",
    "####################################\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "#          Preparing Data          #\n",
    "####################################\n",
    "# 1. data.Field()\n",
    "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
    "LABELS = data.LabelField()\n",
    "\n",
    "# 2. data.TabularDataset\n",
    "train_data, test_data = data.TabularDataset.splits(path=dataset_path,\n",
    "                                                   train=\"train.tsv\",\n",
    "                                                   test=\"test.tsv\",\n",
    "                                                   fields=[('labels', LABELS), ('text', TEXT)],\n",
    "                                                   format=\"tsv\")\n",
    "\n",
    "# train_data, test_data = datasets.IMDB.splits(TEXT, LABELS)\n",
    "\n",
    "print(\"Number of train_data = {}\".format(len(train_data)))\n",
    "print(\"Number of test_data = {}\".format(len(test_data)))\n",
    "\n",
    "print(\"vars(train_data[0]) = {}\\n\".format(vars(train_data[0])))\n",
    "\n",
    "# 3. Split train_data to train_data, valid_data\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "print(\"Number of train_data = {}\".format(len(train_data)))\n",
    "print(\"Number of valid_data = {}\".format(len(valid_data)))\n",
    "print(\"Number of test_data = {}\\n\".format(len(test_data)))\n",
    "\n",
    "\n",
    "# 4. data.BucketIterator\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               device=device,\n",
    "                                                               sort_key=lambda x: len(x.text))\n",
    "# 5. Build vocab\n",
    "TEXT.build_vocab(train_data)\n",
    "# unk_init=torch.Tensor.normal_)\n",
    "LABELS.build_vocab(train_data)\n",
    "print(\"vars(train_data[0]) = \", vars(train_data[0]))\n",
    "\n",
    "# 5.1 (Optional) If build vocab with pre-trained word embedding vectors\n",
    "# TEXT.build_vocab(train_data,\n",
    "#                  vectors=\"glove.6B.100d\")\n",
    "\n",
    "\n",
    "####################################\n",
    "#          Build the Model         #\n",
    "####################################\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # 2. RNN layer\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        # 3. Linear layer\n",
    "        self.fc = nn.Linear(in_features=hidden_dim * 2,\n",
    "                            out_features=output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        # 1. Embedding\n",
    "        # text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # 2. Pack sequence\n",
    "        # embedded = [sent len, batch size, embed size]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n",
    "\n",
    "        # 3. RNN\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        # 4. Unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        # output = [sent len, batch size, hid dim * num_directions]\n",
    "        # output over padding tokens are zero tensors\n",
    "\n",
    "        # hidden = [num_layers * num_directions, batch size, hid dim]\n",
    "        # cell = [num_layers * num_directions, batch_size, hid dim]\n",
    "\n",
    "        # 5. Concat the final forward (hidden[-2, :, :]) and backward (hidden[-1, :, :])\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        # hidden = [batch size, hid dim * num_directions]\n",
    "\n",
    "        # return self.fc(hidden.squeeze(0)).view(-1)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Parameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABELS.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROUPOUT = 0.5\n",
    "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "print(\"INPUT_DIM = {}\".format(INPUT_DIM))\n",
    "print(\"OUTPUT_DIM = {}\".format(OUTPUT_DIM))\n",
    "print(\"TEXT.pad_token = {}\".format(TEXT.pad_token))\n",
    "# print(\"PAD_IDX = {}\".format(PAD_IDX))\n",
    "\n",
    "# Create an RNN instance\n",
    "model = RNN(vocab_size=INPUT_DIM,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            n_layers=N_LAYERS,\n",
    "            bidirectional=BIDIRECTIONAL,\n",
    "            dropout=DROUPOUT)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"The model has {} trainable parameters\".format(count_parameters(model)))\n",
    "\n",
    "\n",
    "####################################\n",
    "#          Train the Model         #\n",
    "####################################\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    correct_pred = 0\n",
    "\n",
    "    # rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    predicted_labels = (torch.sigmoid(preds)).long()\n",
    "    print(\"predicted_labels = {}\".format(predicted_labels))\n",
    "    correct_pred += (predicted_labels == y.long()).sum()\n",
    "\n",
    "    # correct = (rounded_preds == y).float()\n",
    "    # acc = correct.sum() / len(correct)\n",
    "    return correct_pred\n",
    "\n",
    "# Train\n",
    "NUM_EPOCHS = 10\n",
    "total_step = len(train_iter)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = []\n",
    "    for i, batch in enumerate(train_iter):\n",
    "\n",
    "        text, text_lengths = batch.text\n",
    "        y = batch.labels\n",
    "        # print(\"y = {}\".format(y))\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(text, text_lengths)\n",
    "        # print(\"y_pred = {}\".format(y_pred))\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n",
    "\n",
    "    print(\"total_loss = {}\\n\".format(total_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxBJ1nLLKeuY",
    "colab_type": "text"
   },
   "source": [
    "## 2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "toqFXumdKbrC",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "7d1a33ab-d48c-4893-8da8-51f62ccaf500",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.573792145363E12,
     "user_tz": -480.0,
     "elapsed": 13893.0,
     "user": {
      "displayName": "Yantong Laii",
      "photoUrl": "",
      "userId": "11292323517047847832"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Avg. Loss: 0.010766053978204727, Accuracy: 83.752%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct = 0\n",
    "avg_loss = 0.0\n",
    "for i, batch in enumerate(test_iter):\n",
    "    text, text_lengths = batch.text\n",
    "\n",
    "    y = batch.labels\n",
    "        \n",
    "    # Forward pass\n",
    "    y_pred = model(text, text_lengths)\n",
    "    loss = criterion(y_pred, y)\n",
    "    avg_loss += loss.item()\n",
    "\n",
    "    # _, pred = torch.max(output.data, 1)\n",
    "    pred = torch.argmax(y_pred.data, dim=1)\n",
    "    total_correct += (pred == y).sum().item()\n",
    "\n",
    "avg_loss = avg_loss / len(test_data)\n",
    "print(\"Test Avg. Loss: {}, Accuracy: {}%\"\n",
    "        .format(avg_loss, 100 * total_correct / len(test_data)))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN_sentiemnt_custom_dataset.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
